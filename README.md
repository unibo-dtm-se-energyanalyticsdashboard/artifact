```markdown
# Energy Data Analytics System (ENTSO-E)

A compact, course-compliant Python project that **ingests** electricity data from the ENTSO-E Transparency Platform, **stores** it in PostgreSQL, and **visualizes** insights with a Dash dashboard.

This repository follows your course template conventions (report/artifact separation, CI, semantic commits) and uses **Poetry** for dependency management.

---

## 1) Highlights

- **Data ingestion (ETL)** via `entsoe-py` (generation mix, consumption, cross-border flows).
- **PostgreSQL schema** with upserts and uniqueness constraints (idempotent loads).
- **Two run modes**:
  - `full_2025` — backfill
  - `last_10_days` — rolling refresh (default)
- **Dash dashboard** (KPIs fixed at top + tabbed charts/tables).
- **Config via `.env`**, safe example provided.
- **CI** (syntax + unit tests) with GitHub Actions (pip based; Poetry used locally).

---

## 2) Repository Structure (what each file/folder is)

```
.
├── .github/workflows/
│   ├── check.yml           # Course-like CI: pip install, syntax check, mypy (tolerant), unit tests, coverage upload
│   ├── ci.yml              # Minimal CI (single Python on ubuntu): syntax & tests
│   └── deploy.yml          # Optional release workflow (kept from template; can be wired to semantic-release later)
│
├── artifact/
│   ├── scripts/
│   │   ├── db_init.sh      # Helper: apply SQL schema to the configured PostgreSQL database
│   │   └── ingest.py       # CLI entrypoint for ETL (modes: full_2025 | last_10_days; countries e.g., FR DE)
│   └── sql/
│       └── 01_schema.sql   # DDL: countries, energy_production, energy_consumption, cross_border_flow (+ constraints)
│
├── src/edas/
│   ├── __init__.py         # Package marker
│   ├── config.py           # Loads settings from env (.env): DB_* and ENTSOE_API_KEY
│   ├── pipeline.py         # Orchestrates ETL: computes time windows, calls fetchers, and upserts rows
│   │
│   ├── db/
│   │   ├── __init__.py
│   │   └── connection.py   # SQLAlchemy engine builder (reads DB config from env)
│   │
│   ├── ingestion/
│   │   ├── __init__.py
│   │   ├── entsoe_client.py # ENTSO-E queries (consumption, generation mix, cross-border flows) + timezone handling
│   │   └── upsert.py        # Bulk upsert functions for the three fact tables
│   │
│   └── dashboard/
│       ├── __init__.py
│       ├── queries.py      # SQL/aggregation helpers used by the dashboard
│       └── app.py          # Dash UI: fixed KPIs header + tabs (Time Series, Production Mix, Flows, Heatmap, Tables)
│
├── tests/
│   └── test_smoke.py       # Minimal smoke test (kept to satisfy CI)
│
├── .env                    # Local secrets/config (NOT committed) – you create this from the example
├── .env.example            # Safe example of required variables
├── .gitignore
├── CHANGELOG.md            # Generated/updated during releases (if you enable semantic-release)
├── LICENSE                 # Apache-2.0 (as in template)
├── poetry.toml             # Poetry configuration (virtualenvs.in-project = true recommended)
├── pyproject.toml          # Project metadata + Poetry dependencies & scripts
├── poetry.lock             # Locked dependency graph (generated by poetry install)
├── renovate.json           # Renovate configuration (dependency updates)
├── requirements.txt        # Used by CI to bootstrap Poetry (or basic pip installs, depending on workflow)
└── release.config.mjs      # Semantic-release config (kept from template; optional in this project)


````

> **Why two CI files?**  
> - `check.yml` mirrors the course template (multi-step checks; tolerant mypy).  
> - `ci.yml` is a **lean** single-matrix check. You can keep both (only one is strictly necessary).

---

## 3) Prerequisites

- **Python** 3.10 (recommended for lib compatibility)  
- **Poetry** 2.x:
  ```bash
  pip install poetry
````

* **PostgreSQL** 14+ (running locally)
* **ENTSO-E API key** (Transparency Platform)

---

## 4) Configuration

Create `.env` in the repo root (next to `pyproject.toml`) from `.env.example`:

```dotenv
# Database
DB_HOST=localhost
DB_PORT=5432
DB_NAME=energy_analytics
DB_USER=postgres
DB_PASSWORD=your_password

# ENTSO-E
ENTSOE_API_KEY=your_entsoe_api_key
```

> On Windows PowerShell, if script execution is blocked:
>
> ```powershell
> Set-ExecutionPolicy -Scope CurrentUser RemoteSigned
> ```

---

## 5) Install & Dev Environment

```bash
# Use Python 3.10 for Poetry environment
poetry env use 3.10

# Install deps
poetry install
```

> Poetry is configured (via `poetry.toml`) to create `.venv/` inside the project (easy to activate/clean).

---

## 6) Database Setup

Create DB (if needed):

```bash
psql -U postgres -h localhost -c "CREATE DATABASE energy_analytics;"
```

Apply schema:

```bash
psql -U postgres -h localhost -d energy_analytics -f artifact/sql/01_schema.sql
```

Or use the helper:

```bash
bash artifact/scripts/db_init.sh
```

---

## 7) Ingestion (ETL)

Run from project root:

```bash
# Rolling last 10 days (default)
poetry run python artifact/scripts/ingest.py --mode last_10_days --countries FR DE

# Full backfill for 2025
poetry run python artifact/scripts/ingest.py --mode full_2025 --countries FR DE
```

**What it does**

* Loads **consumption** and **generation mix** for FR/DE.
* Loads **cross-border flows** for FR/DE with declared neighbors (only if neighbor zones exist in `countries` table).
* All timestamps normalized to **UTC**, idempotent upserts (unique constraints prevent duplicates).

---

## 8) Dashboard

Start the app:

```bash
poetry run python -m edas.dashboard.app
```

Open: [http://127.0.0.1:8050](http://127.0.0.1:8050)

**Sections**

* **KPIs** (fixed header): totals, daily/weekly/monthly averages, net balance, energy mix.
* **Tabs**:

  1. *KPIs & Time Series* — consumption vs production lines
  2. *Production Mix* — stacked area by source
  3. *Cross-Border Flows* — net bars by corridor
  4. *Hourly Heatmap* — weekday × hour consumption matrix
  5. *Tables* — daily aggregates & flows table

---

## 9) Testing

Minimal smoke test exists. Run:

```bash
poetry run python -m unittest discover -v -s tests
```

> CI workflows (`check.yml`, `ci.yml`) also run syntax/tests on push/PR.

---

## 10) Branching & Commit Convention

* Branching:

  * `dev` → integration branch
  * `feat/...` → feature branches (e.g., `feat/dashboard-base`)
* Commits: **Conventional Commits**, e.g.
  `feat(ingestion): add last_10_days mode`
  `fix(dashboard): correct tz conversion`

> This pairs well with semantic-release if/when you enable automated versioning.

---

## 11) Releases (optional)

`deploy.yml` + `release.config.mjs` are preserved from the course template.
If you want automated releases:

1. Add `RELEASE_TOKEN`, `PYPI_USERNAME`, `PYPI_PASSWORD` (API token) as repo secrets.
2. Ensure commit messages follow Conventional Commits.
3. Push to `main`/`master` per your release policy.

---

## 12) License

[Apache-2.0](./LICENSE)

---

## 13) Troubleshooting

* **DB auth failed** → verify `.env` (`DB_*`) and local PostgreSQL role/password.
* **ENTSO-E 400/429** → reduce time windows, check API key, respect rate limits.
* **PowerShell cannot activate venv** → `Set-ExecutionPolicy -Scope CurrentUser RemoteSigned`
* **Empty charts** → first run ETL to populate the DB, then refresh the dashboard.

```

::contentReference[oaicite:0]{index=0}
```
