---

#  Energy Analytics Dashboard

A modular Python project for data ingestion, transformation, and visualization of European energy data using **Poetry**, **Dash**, and **GitHub Actions**.

---

##  Overview

This project follows a clean and extensible architecture designed for automation, reproducibility, and deployment.
It integrates with **ENTSO-E APIs**, ingests energy data into a **PostgreSQL database**, and provides an interactive **Dash dashboard** for analytics and visualization.

---

##  Main Features

* Modular ETL (Extract–Transform–Load) pipeline
* Database connection and schema initialization (PostgreSQL)
* Automatic orchestration of ingestion tasks
* Interactive Dash-based dashboard for analytics
* Continuous Integration and Deployment via GitHub Actions
* Dependency management with Poetry
* Automated versioning and dependency updates

---

##  Project Structure

```text
.
├── .github/workflows/
│   ├── check.yml           # Course-like CI: pip install, syntax check, mypy (tolerant), unit tests, coverage upload
│   ├── ci.yml              # Minimal CI (single Python on ubuntu): syntax & tests
│   └── deploy.yml          # Optional release workflow (kept from template; can be wired to semantic-release later)
│
├── artifact/
│   ├── scripts/
│   │   ├── db_init.sh      # Helper: apply SQL schema to the configured PostgreSQL database
│   │   └── ingest.py       # CLI entrypoint for ETL (modes: full_2025 | last_10_days; countries e.g., FR DE)
│   └── sql/
│       └── 01_schema.sql   # DDL: countries, energy_production, energy_consumption, cross_border_flow (+ constraints)
│
├── src/edas/
│   ├── __init__.py         # Package marker
│   ├── config.py           # Loads settings from env (.env): DB_* and ENTSOE_API_KEY
│   ├── pipeline.py         # Orchestrates ETL: computes time windows, calls fetchers, and upserts rows
│   │
│   ├── db/
│   │   ├── __init__.py
│   │   └── connection.py   # SQLAlchemy engine builder (reads DB config from env)
│   │
│   ├── ingestion/
│   │   ├── __init__.py
│   │   ├── entsoe_client.py # ENTSO-E queries (consumption, generation mix, cross-border flows) + timezone handling
│   │   └── upsert.py        # Bulk upsert functions for the three fact tables
│   │
│   └── dashboard/
│       ├── __init__.py
│       ├── queries.py      # SQL/aggregation helpers used by the dashboard
│       └── app.py          # Dash UI: KPIs header + tabs (Time Series, Production Mix, Flows, Heatmap, Tables)
│
├── tests/
│   └── test_smoke.py       # Minimal smoke test (kept to satisfy CI)
│
├── .env                    # Local secrets/config (NOT committed) – you create this from the example
├── .env.example            # Safe example of required variables
├── .gitignore
├── CHANGELOG.md            # Generated/updated during releases (if you enable semantic-release)
├── LICENSE                 # Apache-2.0 (as in template)
├── poetry.toml             # Poetry configuration (virtualenvs.in-project = true recommended)
├── pyproject.toml          # Project metadata + Poetry dependencies & scripts
├── poetry.lock             # Locked dependency graph (generated by poetry install)
├── renovate.json           # Renovate configuration (dependency updates)
├── requirements.txt        # Used by CI to bootstrap Poetry (or basic pip installs, depending on workflow)
└── release.config.mjs      # Semantic-release config (kept from template; optional in this project)
```

---

##  Setup Instructions

### 1 Install dependencies

```bash
pip install -r requirements.txt
poetry install
```

### 2 Initialize the database

```bash
bash artifact/scripts/db_init.sh
```

### 3 Run ingestion (fetch & store data)

```bash
python artifact/scripts/ingest.py
```

### 4 Launch the dashboard

```bash
poetry run python -m src.edas.dashboard.app
```

Then open [http://127.0.0.1:8050](http://127.0.0.1:8050) in your browser.

---

##  Testing

Unit and smoke tests are automatically run by **GitHub Actions** for each commit and pull request.
You can run them locally via:

```bash
poetry run python -m unittest discover -v
```

---

##  CI/CD Integration

* **`check.yml`** → Runs syntax, linting, and unit tests
* **`ci.yml`** → Minimal CI workflow on Ubuntu
* **`deploy.yml`** → Optional automated release on PyPI (using semantic-release)

---

##  Versioning & Releases

Semantic versioning is handled automatically through commit messages following [Conventional Commits](https://www.conventionalcommits.org/).
New releases are published to **PyPI** whenever changes are merged into the `main` branch.

---

##  License

Distributed under the **Apache License 2.0**.
See [`LICENSE`](LICENSE) for more details.

---
